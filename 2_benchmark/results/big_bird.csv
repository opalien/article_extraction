Model_true,Model_predicted,Parameters_true,Parameters_predicted
Incoder-6.7B,,6700000000.0,
Gemma 3 QAT 27B,,27000000000.0,
Improved motif-scaffolding with SE(3) flow matching,,16800000.0,
Qwen-Audio-Chat,,8460000000.0,
Jais,,13000000000.0,
Seedance 1.0,,,
Word2Vec (large),,692000000.0,
Stacked Denoising Autoencoders,,,
Llama 3.3 70B,,70000000000.0,
Infinity (无涯),,,
Humanoid Locomotion,,8000000.0,
GRITLM 8x7B,,46700000000.0,
Diffusion Renderer,,1100000000.0,
Taiyi-Stable Diffusion,,1000000000.0,
SD-Turbo,,,
Spatially-Sparse CNN,,,
UnifiedQA,,11000000000.0,
Yi-34B,,34000000000.0,
XLNet,,340000000.0,
Multiscale deformable part model,,,
OPT-2.7B,,2700000000.0,
RoBERTa Large,,355000000.0,
OpenThaiGPT v1.0.0 (7B),,6810000000.0,
Baichuan2-53B,,53000000000.0,
Dropout (ImageNet),,,
Mistral 7B + OVM,,7000000000.0,
Cross-Lingual POS Tagger,,,
Textual Imager,,,
TSN,,,
AlphaFold 3,,,
WaveNet,,,
WeNet (PTB),,23000000.0,
o1-preview,,,
Alleviated TOI 10 (WT2),,,
Llemma 7B,,7000000000.0,
Deep CNN + COTS,,5006000.0,
HiDream-I1,,18000000000.0,
Step-Audio-Chat 130B,,130000000000.0,
Genie 2 (bio),,15700000.0,
Imagen 4 ultra,,,
Amazon Titan Text Lite,,,
ImageBind,,932000000.0,
MedGemma 27B,,27000000000.0,
CausalLM 34B β,,34400000000.0,
"We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and 720p, while the final SFT dataset is built through manual annotation.",,,
DeepUrfold,,110000000.0,
AFP+FPI (WT2),,13600000.0,
AraELECTRA,,136000000.0,
ChatGPT agent,,,
Pooling CNN (NORB),,268664.0,
LLaMA-65B,,65200000000.0,
BlueLM 175B,,175000000000.0,
MCDNN (MNIST),,2653700.0,
Molmo 72B,,72000000000.0,
DeepHermes 3 - Mistral 24B,,24000000000.0,
4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment,,,
MindLink-32B,,32000000000.0,
Megatron-LM (2.5B),,2500000000.0,
Whale Bioacoustics Model,,,
InternImage,,1080000000.0,
B2T connection (16L),,,
SANA 1.5 4.8B,,4800000000.0,
Very Deep VAEs (ImageNet-64),,125000000.0,
Lumina-Image-2.0,,2600000000.0,
ASE+ACE,,324.0,
GLM 4.5,,355000000000.0,
NeoaPred,,,
PaLM-SayCan,,540000000000.0,
YandexGPT 3,,,
Llama 4 Behemoth (preview),,2000000000000.0,
SeamlessM4T,,2300000000.0,
CT-MoS (WT2),,45000000.0,
Zi Yue,,,
OpenThaiGPT R1 32b / OTG-R1 (32B),,32000000000.0,
Gemini 1.5 Flash 8B,,8000000000.0,
MedBERT,,17000000.0,
Zhiyun Culture LLM (智云文化大模型) ,,,
Kolors 2.0 Image Generation,,,
Granite 3.2 2B,,2000000000.0,
BELLE-7B-0.2M,,7000000000.0,
XGen-7B,,6700000000.0,
BiRNA-BERT,,117000000.0,
XLMR-XXL,,10700000000.0,
ControlNet (SDv2),,,
OCTAVE 3B,,3000000000.0,
Low-Cost Collaborative Network,,,
Reason-ModernColBERT,,150000000.0,
U-Net,,37676160.0,
Llama Nemotron Nano 8B,,8000000000.0,
GAIA-2,,8685000000.0,
CELLE-2,,,
Anthropic LM 175B,,175000000000.0,
"MSRA (C, PReLU)",,87048800.0,
Decision tree adaline,,2450.0,
NEC cotomi,,,
SCUBA-D,,,
GTE-ModernColBERT-v1,,149000000.0,
Gemini 2.5 Deep Think,,,
TransFew,,,
FvFold,,9209788.0,
MultiBand Diffusion,,,
Qwen3-4B,,4000000000.0,
Palmyra X5,,,
Discriminator-tuned LSTM,,111920000.0,
code2vec,,,
P-Mistral,,7000000000.0,
MuseNet,,2038431744.0,
Megatron-LM (355M),,355000000.0,
Alphaflow,,,
DeepSeek-Prover-V2-671B,,671000000000.0,
Amazon Transcribe,,,
Qwen2.5-32B,,32500000000.0,
Claude Sonnet 4.5,,,
Vine copula (crime),,,
GPT2-Large+LHOPT,,760000000.0,
ESM1-43M,[Image 1: Icon for www.pnas.org](https://www.pnas.org/favicon.ico,42600000.0,.
MLN-ASR,,10000.0,
AudioLM,,1500000000.0,
AInno-75B 2.0,icon-small-nocollect.png,75000000000.0, 10%
Zhiyun Culture LLM (智云文化大模型),,,
Imagen,,7762000000.0,
AbLang (heavy sequences),,355000000.0,
NV-Embed-v1,,7000000000.0,
GigaChat 2 MAX,,,
GPT-2 Medium (FlashAttention),,355000000.0,
imagen 4 fast,,,
Granite 3.0 2B,,2500000000.0,
Flow++ (CIFAR10),,31400000.0,
LongCat-Flash,,560000000000.0,
Cosmos-Predict2-14B-Text2Image,,14000000000.0,
Transformer-XL Large + Phrase Induction,,257000000.0,
GigaChat 2 Pro,,,
Marey,,,
Jueqing LLM (觉卿大模型),,,
SenseChat 4.0,,,
GPT-NeoX-20B,,20000000000.0,
GShard (600B),,600000000000.0,
EDSR,,,
BGE-M3 Embedding,Failure,335000000.0, 404
Nemotron-H 47B,,47000000000.0,
Protst,,,
Luminous-supreme, Aleph Alpha Docs,70000000000.0,Image 1: Aleph Alpha Logo
Vicuna-33B-v1.3,,33000000000.0,
Falcon3-7B,,7000000000.0,
NPLM (Brown),"
dl.acm.org",4124233.0,.
Wutong,,,
PointNet,,,
VILA1.5-13B,,13493916736.0,
DeepSeekMoE-16B,,16000000000.0,
Llama SEA-LION V2 8B,,8000000000.0,
Galactica,,120000000000.0,
Japanese-GPT-1B,,1300000000.0,
BPL,"
science.sciencemag.org",,.
genCNN + dyn eval,,8000000.0,
Doubao Image-to-Image generation model,,,
InternVL2-Llama3-76B,,76000000000.0,
PMLM-large,,250000000.0,
Firefly Image 4 Ultra,,,
DrugFormer,,21660193.0,
Apollo 1.5B,,1500000000.0,
DeepL LLM,,,
Dropout (MNIST),,5594010.0,
STORM-B/8,,100598707.0,
BlueLM 130B,,130000000000.0,
EXAONE 2.0,,300000000000.0,
PPLX-70B-Online,,70000000000.0,
M4-50B,,50000000000.0,
CT-MoS (PTB),,24000000.0,
mPLUG-Owl2,,7120000000.0,
XY-LENTXL,,2000000000.0,
GPT-3 13B,,12850000000.0,
MegaSyn,,,
Knowledge distillation student model,,84000000.0,
MSA Transformer,,100000000.0,
OmniNA,,1700000000.0,
Voicebox / VB-En,,358000000.0,
VGG16,,138000000.0,
Refined Part Pooling,,,
Genie 3,,,
GemNet-T (OC20),,1900000.0,
X-Portrait,,,
FASHN v1.5,,,
ProtSSN,,1467000000.0,
Go-explore,,,
Deconvolutional Network,,,
Layer Normalization: Handwriting sequence generation,,3700000.0,
ConvS2S (ensemble of 8 models),,,
LLaVA-OV-72B,,72000000000.0,
EVA-CLIP (EVA-02-CLIP-E/14+)																											,,5000000000.0,
ResNet-152 (ImageNet),,60200000.0,
360gpt2-pro,,,
A.X (Adot) 7B,,7000000000.0,
VASA-1,,229000000.0,
DeBERTaV3large,,418000000.0,
"Digivio (迪智伟奥DIGIVIO)


",,,
Vicuna-13B v0,,13000000000.0,
VQ-VAE-2 (FFHQ),,,
Transformers in music recommendation,,,
Gen-1,,,
Typhoon2-Audio,,9688000000.0,
HiDream Foundation Model 3.0,,10000000000.0,
ProlificDreamer,,,
Rainbow DQN,,,
Image-01,,,
Spark 4.0,,,
Search-Proven Best LSTM,,20000000.0,
WGAN (Wasserstein GAN),,,
BITTERS,,650000000.0,
CodonMPNN,,,
LSTM (WT2),,,
DeepSeek-Prover-V2-7B,,7000000000.0,
TrellisNet,,180000000.0,
EVI,,,
Genie,,10700000000.0,
CodeGen2.5,,7000000000.0,
DeciCoder-6B,,6000000000.0,
A3C FF hs,,,
Amazon Q Developer,,,
MusicLM,,860000000.0,
Zamba2-7B,,7000000000.0,
ELMo,,94000000.0,
SO3LR,,,
BP-DBN,,18030592.0,
SPN-4+KN5,,5000000.0,
GLM-4,,,
Qwen2-VL-72B,,72000000000.0,
π0.5 (pi-0.5),,3300000000.0,
Variational Lossy Autoencoder (VLAE) MNIST,,,
LinOSS,,,
Alibaba-NLP (mGTE),,304000000.0,
TensorReasoner,,,
MiniMax-M1-40k,,456000000000.0,
TeleChat2-3B,,3000000000.0,
Reka Core,,67000000000.0,
Falcon-7B,,7000000000.0,
ProtT5-XL-U50,,3000000000.0,
T-Pro,,32000000000.0,
Shuka-1,,9560000000.0,
AraBERT LArge v2,,371000000.0,
Wan 2.1 14B I2V,,14000000000.0,
base LM+GNN (WT103),,247000000.0,
Multi-cell LSTM,,7200000.0,
II-Medical-8B,Why Specialized Medical Models Matter,8000000000.0," 103,031"
Me Llama 13B,,13000000000.0,
AlphaGo Fan,,8209984.0,
GRU + p-tHSM (pretrain via Brown) (PTB), Forbidden,, 403 Client Error: Forbidden
MolmoAct-7B-D,,7000000000.0,
RNN for 1B words,,20000000000.0,
Palmyra Vision,,,
ViT-22B,,21743000000.0,
wave2vec 2.0 LARGE,,317000000.0,
PoE MNIST,,3925310.0,
Qwen-VL-Max,,7000000000.0,
Golem,,,
Cerebras-GPT-13B,,13000000000.0,
InternLM3,,8000000000.0,
EvolMPNN,,,
Movie Gen Audio,,13000000000.0,
PaliGemma 2 3B Mix 224,,2920000000.0,
3-ensemble of Self-ensembles on CIFAR-100,,60200000.0,
Phi-4 Mini,,3800000000.0,
YaYi 2.0,,30000000000.0,
Gemini Embedding,,,
KwooVa,,8000000000.0,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",,35000000.0,
EGRU (WT2),,74000000.0,
HelixFold,,,
RNNLM + Dynamic KL Regularization (WT2),,87600000.0,
OREAL 7B,,7000000000.0,
MiniCPM-3-4B,,4000000000.0,
StableLM-Base-Alpha-7B,,6890209280.0,
RSM,,,
LTX-Video-0.9.1. 2B,,1900000000.0,
UDSMProt,,28303800.0,
MAP-Neo,,7000000000.0,
Skywork-OR1-32B,,32000000000.0,
Transformer-XL-ptb,,24000000.0,
Boss (DARPA Urban Challenge),,,
Step-1X,,,
GPT-2 (fine-tuned with HYDRA),,1540000000.0,
babbage-002,,,
Fisher Vector image classifier,,,
OPT-66B,,66000000000.0,
DeepLabV3,,,
Naive Bayes,,,
Optimized Single-layer Net,,,
Rita-XLarge,,1200000000.0,
CPM-Bee,,10000000000.0,
Qwen2-Math-1.5B,,1500000000.0,
BERT-RBP,,110000000.0,
Hunyuan-TurboS,,560000000000.0,
TOME,,220000000.0,
DALL-E,,12000000000.0,
SciBERT,,110000000.0,
LF-MMI,,16600000.0,
MolPhenix,,38700000.0,
CaLM,,86000000.0,
Kimi-VL,,16000000000.0,
MPDF,,,
dnaGrinder,,63600000.0,
Doubao Text-to-Speech Model,,,
LBSTER,,67000000.0,
GPT-4.5,,,
Zero-shot Monocular Scene Flow (ZeroMSF),,,
XLM-RoBERTa,,550000000.0,
Order-Embeddings of Images and Language,,,
WizardCoder-15.5B,,15500000000.0,
SEA-LION V3 Llama3.1 70B,,70000000000.0,
SambaLingo-Thai-Chat (7B),,6950000000.0,
Vega v2,,6000000000.0,
Stable LM 2 12B,,12143605760.0,
Lyria RealTime,,,
Coconut,,,
Elyza,,13000000000.0,
bpRNA-align,,,
trRosetta,Markdown Content,,".pnas.org/doi/10.1073/pnas.1914677117

Warning: Target URL returned error 403"
GPT-3 Large,,760000000.0,
Tencent Search LLM (腾讯搜索大模型),,,
TeleChat-3B,,3000000000.0,
Detic,,88000000.0,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),,26000000.0,
Transformer-C,,148000000.0,
Amazon Nova Premier,,,
ProtGPT2,,738000000.0,
Swift,,56804.0,
Guanaco-65B,,65000000000.0,
Kling 1.5 Pro,kling-1-5-ai-video-generator,,finally-here-with-major-upgrades
Whisper,,1550000000.0,
Qwen3-8B,,8200000000.0,
Stable Point Aware 3D (SPAR3D),,,
Sora Turbo,,,
Depth Anything V2 Large,,335300000.0,
High Performance CNN (NORB),,4878300.0,
Belle-whisper-larger-v3-turbo-zh ,,,
ANN Eye Tracker,,5620.0,
Emu3,,8000000000.0,
Sandstorm (DARPA Grand Challenge I),,,
Sonar,,70000000000.0,
Flan T5-XXL + BLIP-2,,12100000000.0,
Gopher (280B),,280000000000.0,
Mixture-of-Depths,,3000000000.0,
Evo 2 40B,,40300000000.0,
Tianxi-72B,,72000000000.0,
BASIC-L + Lion,,3070000000.0,
4 layer QRNN (h=2500),,151000000.0,
GLM-4 (0520),,,
TeleChat2-7B,,7000000000.0,
OpenDiLoCo 150M,,150000000.0,
Stable Diffusion 1.4,,,
gLM,,1000000000.0,
GLM-4.5V,,108000000000.0,
Adaptive Subgrad,"
dl.acm.org",,.
AdvSoft + 4 layer QRNN + dynamic evaluation (WT103),,,
Bankruptcy-NN,,36.0,
Phi-4-Reasoning-plus,,14000000000.0,
Quantized ADMM,,,
Gemini Robotics,,,
Pika 2.2,,,
Nemotron-3-8B,,8000000000.0,
MiniMax Speech-01-turbo (T2A-01-turbo),,,
NMT Transformer 437M,,437700000.0,
Tulu 3 8B,,8000000000.0,
UniDiffuser (多模态大模型),,,
DLRM-2020,,100000000000.0,
METL-Global,,50000000.0,
MultiVerse 70B,,72000000000.0,
ONE-PEACE,,4000000000.0,
MoLFormer-XL,,,
BLOOM-560M,,560000000.0,
Gauss2,,,
aLSTM(depth-2)+RecurrentPolicy (PTB),,24000000.0,
ERNIE-4.5-VL-28B-A3B,,28000000000.0,
New Intelligent Q&A,,,
PixelSNAIL (CIFAR 10),,,
LLaMA-13B,,13000000000.0,
VideoMAE V2,,1000000000.0,
Meena,,2600000000.0,
RiNALMo,,650000000.0,
Llama 2-7B,,7000000000.0,
Sparse Vision Encoding,,,
Jais-30b (phase 1)																												 ,,30000000000.0,
Xingrui AI (星睿AI),,100000000000.0,
SparseOPT-13B,,13000000000.0,
GPT-4b micro,,,
SEA-LION V3 Llama3.1 8B,,8000000000.0,
PULI GPTrio,,6700000000.0,
Hierarchical Cognitron,,9315.0,
CT-MoS + DynamicEval (WT2),,45000000.0,
PepPrCLIP,,,
GPT-2 (1.5B),,1500000000.0,
Ministral 8B,,8000000000.0,
Japanese StableLM Base Alpha 7B,,7000000000.0,
Mercury,,,
Luma Photon Flash,,,
LLaMA-7B (LoRA finetuned),,7000000000.0,
CNN committee (traffic sign),,1388800.0,
Genie 2,,,
WeNet (Penn Treebank),,23000000.0,
Transformer,,213000000.0,
Flan-T5 11B,,11000000000.0,
Qwen2-57B-A14B,,57000000000.0,
NAS with base 8 and shared embeddings,,54000000.0,
GBERT-Large,,335000000.0,
Llama 4 Maverick,,400000000000.0,
Whisper v2,,1550000000.0,
improved U-Net for chest X-ray images segmentation,,,
Speechmatics Enhanced,,,
voyage-code-2,,,
YiSu, 39,, 48
Youyuanjian (邮远见),,,
Multitask Unified Model (MUM),,,
RECONTRA-categorized,,66780.0,
AFM-server,,,
LingoWhale-8B,,8000000000.0,
TRIMELMlong (150M),,150000000.0,
Athene-V2,,72000000000.0,
KwaiYiiMath,,13000000000.0,
Siamese-TDNN,,744.0,
TransfoRNN(d=1024)(2-layer) (PTB),,97600000.0,
"Segatron XL large, M=384",,257000000.0,
SambaLingo-Thai-Chat-70B,,70000000000.0,
Aya Expanse 32B,,32300000000.0,
Print Recognition Logic,,,
Qwen2.5-3B,,3090000000.0,
Smaug-72B,,72000000000.0,
Transformer Large + HCP,,257000000.0,
ViT-Huge/14,,632000000.0,
ZhihaiTu AI (知海图),,,
Pika 2.1,,,
IDEFICS-9B,,9000000000.0,
Amazon Nova Canvas,,,
AI Q&A Robot,,13000000000.0,
Attend-Infer-Repeat,,82130304.0,
MiMo-7B-Base,,7000000000.0,
Photo-Geometric Autoencoder,,,
Cygnet,,8000000000.0,
Idefics2,,8000000000.0,
"Mamba 2, 2.7B",,2700000000.0,
Hailuo T2V-01-Director,,,
xLSTM 1.4B,,1422600000.0,
LIMA,,65000000000.0,
SigLIP 400M,,400000000.0,
ProtBERT-UniRef,,420000000.0,
YOLOv10-X,,29500000.0,
DeepSeek-R1-Distill-Qwen-7B,,7000000000.0,
GRU + p-tHSM (pretrain via Brown) (WT2), Forbidden,, 403 Client Error: Forbidden
Moonshot-v1,,,
Transformer-XL + RMT,,247000000.00000003,
Gen-3 Alpha Turbo,,,
IDPFold,,17800000.0,
RT-2,,55000000000.0,
Janus-Pro-1B,,1000000000.0,
Feedforward NN,,7082000.0,
Doubao-pro,,500000000000.0,
BetterBodies,,,
Hunyuan-Large,,389000000000.0,
ProgressiveGAN,,,
Mobile V-MoEs,,,
Parti,,20000000000.0,
Baichuan4,,,
GPT-Neo-1.3B,,1300000000.0,
EXAONE Path 2.0,,175000000.0,
Japanese-LM-3.6B,,3600000000.0,
GPT-3 6.7B,,6660000000.0,
LSTM+Noise(Beta),,51000000.0,
MIF-ST,,,
Llama Nemotron Super v1.5,,49000000000.0,
ESM1-670M (UR100),[Image 1: Icon for www.pnas.org](https://www.pnas.org/favicon.ico,669200000.0,.
AWD-LSTM-DOC (fin) (23M),,23000000.0,
VideoPoet,,8000000000.0,
GPT-4,,1800000000000.0,
GOAT,,3472816.0,
Doubao Speech Recognition Model,,,
AlphaGeometry,,151000000.0,
Jamba 1.6 Mini,,52000000000.0,
TF-LM-discourse LSTM (PTB),,,
GPT-J-6B,,6053381344.0,
MiniCPM-2.4B,,2442057984.0,
ISS,,11100000.000000002,
Code Llama-13B,,13000000000.0,
"Ovis2 16B
",,16000000000.0,
DoMINO,,,
fastText,,,
InternVL2_5-26B,,25500000000.0,
GPT2+CoreLM+Fine-Tuning,,132000000.0,
Boltz-1,,,
DL scaling speech,,193000000.0,
Uni-Med,,8800000000.0,
BEIT-3,,1900000000.0,
Phi-3.5-MoE,,60800000000.0,
UniGPT-mMed,,,
SmolLM-1.7B,,1710000000.0,
LongVILA-7B,,7000000000.0,
PanGu-α,,207000000000.0,
DiffDock,,20240000.0,
iGPT-XL,,6801000000.0,
3DMM-CNN,,44500000.0,
TA-CNN,,706048.0,
Xiaomi Pengpai Image (小米澎湃图像),,,
GPT3-6.7B + muP,,6700000000.0,
FLUX.1 Kontext [pro],,,
Make-A-Scene,,4000000000.0,
ParetoDrug,,,
BIDAF,,2600000.0,
HRA,,,
Refact-1.6B,,1600000000.0,
LTX-Video-0.9.5. 2B,,1900000000.0,
XuanYuan 2.0,,176200000000.0,
TxGemma 2B,,2600000000.0,
Eleven Monolingual v1,,,
Nanbeige2-16B-Chat, Hugging Face,15800000000.0,Terms of service
Heuristic problem solving for AI,,,
CPM-Large,,2600000000.0,
ByteDance Seaweed,,,
ether0,,24000000000.0,
Wutong 2.0,,,
BIG-G 137B,,137000000000.0,
KwooLa,,14000000000.0,
TAIDE LX-13B,,13000000000.0,
T2R + Pretrain,,668893184.0,
360zhinao2-o1,"

",, 403
MahLool,acs.jcim,,pubs.acs.org/doi/full/10.1021/acs.jcim.2c01317
Perfusion,,,
Kosmos-1,,1600000000.0,
VQGAN + CLIP,,,
ProteinDT,,,
TransE,,942000000.0,
nekomata-14b,,14200000000.0,
NIO World Model (蔚来大模型),,,
RFM-1,,8000000000.0,
BRIA 3.1,,4000000000.0,
Veo 3,,,
Xingchen LLM (淘宝（中国）软件有限公司),,,
AWD-LSTM-DRILL + dynamic evaluation† (WT2),,34000000.0,
NAS+ESS (156M),,156000000.0,
"LSTM (Hebbian, Cache, MbPA)",,530442240.0,
Step-Audio-TTS-3B,,3000000000.0,
Baichuan1-7B,,7000559616.0,
Ovis2.5 2B,,2000000000.0,
Dexterous In-Hand Manipulation [control policy],,3181588.0,
Cohere Command A Reasoning,,111000000000.0,
OpenELM-450M,,450000000.0,
Multi-scale Dilated CNN,,,
RNN+weight noise+dynamic eval,,54000000.0,
OpenBioLLM-Llama3-70B,,70000000000.0,
ReLU-Speech,,101706240.0,
Granite 3.1 8B,,8100000000.0,
gpt-oss-20b,,20910000000.0,
MiniMax-M1-80k,,456000000000.0,
Quiet-STaR,,,
ProteinGenerator,,,
ERNIE x1 (文心大模型X1),,,
Table Tennis Agent,,185000.0,
Amazon Titan Text Premier,,,
Importance of higher-order epistasis in large protein sequence-function relationships,,,
Pragmatic Theory solution (Netflix 2009),,,
AlphaProteo,,,
Pixtral Large,,124000000000.0,
Llama-2-Chinese 13B,,13000000000.0,
ProxylessNAS,,,
Self-Attention and Convolutional Layers,,29500000.0,
RNN + char3-MS-vec,,175000000.0,
n-gram LM,,,
DAC-CSR,,,
Skiff LLM (一叶轻舟大语言模型),,540000000000.0,
Discriminator Guidance,,,
DTI-LM,,,
Stable Diffusion 3.5 Large,,8100000000.0,
DNA Fine-Tuned Language Model (DFLM),,,
GLM-10B-bidirectional,,10000000000.0,
Llama 3.2 90B,,88600000000.0,
Gemini 1.0 Pro,,,
GLM-Z1-Rumination-32B-0414,,32000000000.0,
Mistral Large 2.1,,123000000000.0,
MegaScale (Production),,530000000000.0,
Delta RNN (+ full context),,44600000.0,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),,24000000.0,
ALLaM adapted 70B,,70000000000.0,
Llama 3.1-405B,,405000000000.0,
Xunguang,,,
MetaMath 7B (LLaMa finetune),,7000000000.0,
SeaMoon,,1000000.0,
AFP-Deep,,,
RetinaNet-R101,,53000000.0,
Llama Nemotron Super 49B,,49000000000.0,
ControlNet (SD 3.5 Large) Canny,,,
Fulu Gua (福禄瓜),,,
Mixtral 8x7B,,46700000000.0,
Taiyi (旷视太乙),,,
SenseChat 5.5,,600000000000.0,
Qwen2.5-VL-3B,,3000000000.0,
EquiDock,,,
Konan LLM 13B,,13100000000.0,
AraBERT,,110000000.0,
DeepRelax,,,
GSM,,,
TeleChat,,,
Decision tree (classification),,12000.0,
HLBL,,1846400.0,
WGAN-GP,,,
LTX-Video-0.9.7. 13B ,,13000000000.0,
InternViT-6B,,6000000000.0,
Pythia-1.4b,,1400000000.0,
Flan UL2,,19500000000.0,
Inflection-2.5,,,
Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage,,2176.0,
NCP-VAE (Celeba  HQ),,,
Speaker-independent vowel classification,,3040.0,
"ERNIE-Doc Base (151M, WT103)",,151000000.0,
Nemotron-H 56B,,56000000000.0,
OmniHuman-1,,,
DCTransformer (ImageNet),,736000000.0,
Sequence-based pattern recognition,Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,,.
OPT-175B,,175000000000.0,
Solar-10.7B (Solar Mini),,10700000000.0,
R1 1776,,671000000000.0,
Deep Multitask NLP Network,Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,1500000.0,.
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)",,22000000.0,
Diffractive Deep Neural Network,,8000000000.0,
DistilProtBert,,230000000.0,
PixelCNN,,,
Alleviated TOI 10 (WT103),,,
TokenFlow-t2i,,,
Goat-7B,,7000000000.0,
Kling 2.0 Video Generation,,,
ruDalle: Kandinsky 3.0,,11900000000.0,
Hunyuan Turbo,,,
AMIE (Articulate Medical Intelligence Explorer),,340000000000.0,
Sonar Deep Research,,,
ResNeXt-101 32x48d,,829000000.0,
Chirp 3 Speech-to-Text,,,
Restricted Boltzmann machine for Face Recognition,,,
DnCNN,,,
SenseChat-DataAnalysis V4,,,
Eagle 2,,8930000000.0,
Diffusion-GAN,,,
Yi-Large,,100000000000.0,
AlphaFold,,16340840.0,
Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,,,
Transformer-XL + PowerSGD + L-Greco,,,
PG-SWGAN,,,
EnzymeFlow,,,
K2 Think,,32000000000.0,
Qwen2-VL-7B,,8000000000.0,
Gemma 7B,,8538074112.0,
AntiFormer,,24670596.0,
Unichat-32B-c1,,32000000000.0,
Deep Autoencoders,,139808256.0,
code2seq,,37000000.0,
Multilingual-E5-large,,560000000.0,
Instruct-GPT + Mind's Eye,,176500000000.0,
Llama-Primus-Nemotron-70B,,70000000000.0,
EXAONE 3.5-R 2.4B,,2400000000.0,
TxGemma 27B,,27000000000.0,
AlphaMut,,,
eDiff-I,,9100000000.0,
Cosmos-Predict2-2B-Text2Image,,2000000000.0,
Qwen-72B,,72000000000.0,
AlphaGo Master,,,
LLaVA + LVIS-INSTRUCT4V,,13000000000.0,
Uni-RNA-L16,,169000000.0,
RNA-MSM,,,
HAM-TTS,,800000000.0,
Enhanced Neighborhood-Based Filtering,,,
EI-REHN-1000D,,19000000.0,
GPT-4 Turbo,,,
Orca 2-13B,,13000000000.0,
Devstral Medium,,,
Part-of-sentence tagging model,,,
OpenELM-3B,,3040000000.0,
AlphaMissense,adg7492,93000000.0,.
VILA1.5-40B,,40000000000.0,
Claude 3 Haiku,,,
Fractional Max-Pooling,,27000000.0,
Codestral,,22200000000.0,
Pangu Ultra,,135000000000.0,
llama-3-airoboros-70b-3.3,,70000000000.0,
GPT3-6.7B (rerun of original),,6700000000.0,
KwaiYii 13B,,13000000000.0,
Zip CNN,,9760.0,
Mathstral,,7000000000.0,
Xinyuan (心元大模型),,14000000000.0,
Masked Autoencoders ViT-H,,632000000.0,
Tensorized Transformer (257M),,257000000.0,
BIG LSTM+CNN INPUTS,,,
Kosmos-2,,1600000000.0,
Veo 2,,,
ESM-DBP,,650000000.0,
CodonTransformer,,89600000.0,
EVI 2,,,
Otter,,1300000000.0,
ProLLaMA,,7000000000.0,
Pangu-Weather,,256000000.0,
RBM-tuning,,,
TerraMind,,,
Apertus 8B,,8000000000.0,
Credibilty Network,,324.0,
SenseNova Unified Large Model,,,
Codex,,12000000000.0,
PaLM-2 Bison,,,
Vicuna-7B-v1.3,,7000000000.0,
Projected GAN,,,
Immediate trihead,Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,,.
Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient,,35800000.0,
"GCRN-M1, dropout",,42000000.0,
Spectrally Normalized GAN,,,
UniRep,,18200000.0,
RNAformer,,32000000.0,
GR-2,,230000000.0,
Granite 3.0 8B,,8100000000.0,
MPNNsol,,,
OLMoE,,7000000000.0,
Eve,,15010300.0,
DLDL (PASCAL),,564000000.0,
Ovis1.6-Gemma2-27B,,28900000000.0,
Uni-RNA-L12,,85000000.0,
AstroOne,,70000000000.0,
Weight Decay,,8386.0,
Skip-Thoughts,,,
WizardLM-2 8x22B,,141000000000.0,
STT Conformer-Transducer XL,,600000000.0,
InstructBLIP,,13000000000.0,
STeLLA,,,
Ngram corpus,,,
EVI 3,,,
BaseFold,,,
DolphinGemma,,400000000.0,
LSTM,,10504.0,
Mistral Saba,,24000000000.0,
Walking Minotaur robot,,,
Hybrid H3-125M,,125000000.0,
PolySphere-1,,14000000000.0,
Qwen2.5-Omni 7B,,7000000000.0,
SPN-4,,,
Flux.1 [dev],,12000000000.0,
RNN Baseline,,,
JEST-L++,,,
Octo-Small,,27000000.0,
SAGAN,,,
R-Transformer,,15800000.0,
Nova-2,,,
OuteTTS-0.1-350M," 404 Not Found

",350000000.0,0.1-350m
FAST,,,
MuPIPR,,,
Typhoon2-Vision ,,7000000000.0,
Typhoon2-Vision,,7000000000.0,
NVAE (Celeba HQ),,,
HiFi - NN,,3000000.0,
ELIXR-C,,,
s1.1,,32000000000.0,
Xingchen Multimodal Model (中电信人工智能科技（北京）有限公司),,,
Arctic,,480000000000.0,
MolSnapper,,,
PixelCNN++,,,
Consistency Model (CIFAR-10),,,
Samsung Gauss Code,,,
Fully Convolutional Networks,,,
KnGPT2,,83000000.0,
Falcon Mamba,,7000000000.0,
AdClickNet,Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,,.
OPT-13B,,13000000000.0,
OpenAI Five,,159000000.0,
Base LM + kNN LM + Continuous Cache,,247000000.00000003,
NPLM (AP News),Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,11904264.0,.
Flexi-JEST++,,,
Aramco Metabrain AI,,250000000000.0,
Qwen2.5-Turbo,,,
Hierarchical LM,,,
GLM-4 (0116),,,
GroundingGPT,,7000000000.0,
GPT-2 (774M), HTTPSConnectionPool,774000000.0,supervised_multitask_learners.pdf
Hyena-2 355M,,355000000.0,
MnasNet-A3,,5200000.0,
InternVL2-40B,,40100000000.0,
Tacotron,,,
Xinghai (星海),,,
Web mining + Decision tree recommender,,,
Pythia-12b,,12000000000.0,
Samsung Gauss Image,,,
Grover-Mega,,1500000000.0,
DINOv2,,1140000000.0,
Animate Anyone,,,
LinGan VL (临感VL),,,
Samba 3.8B,,3800000000.0,
Amazon Nova Reel,,,
MS-ensemble-speech-recognition,,3172117056.0,
Qwen3 Embedding,,8000000000.0,
EpiScan,,288915.0,
Gopher (7.1B),,7100000000.0,
Mistral Medium 3,,,
OLMo 2 32B,,32000000000.0,
QVQ-Max,,,
WizardLM-2 7B,,7000000000.0,
NetTalk (transcription)," 502 Bad Gateway

",18629.0," 502 Bad Gateway

"
MEGNet (molecule model),,8720.0,
DataGemma,,27200000000.0,
ALOHA Unleashed,,217000000.0,
DeepSeek LLM 7B,,7000000000.0,
Shanhai 2.0,,,
DQN,,836096.0,
Poro 34B,,34200000000.0,
The Attentive Reader,,,
ContextNet + Noisy Student,,,
ConvNet Processor,,14423.0,
MegaScale (530B),,530000000000.0,
SRU++ Large only 2 attention layers (k=5) (WT103),,225000000.0,
DiscDiff,,,
SenseChat-Vision V4,,30000000000.0,
TD-Gammon,,25000.0,
Eleven Flash v2.5,,,
OpenLLaMA-13B,,13000000000.0,
DALL·E 3,,,
Minimax-Speech-02-HD,,,
Stable Video 4D (SV4D),,,
OPT-1.3B (finetuned),,1300000000.0,
RNN + char4-MS-vec,,226000000.0,
KnoMol,acs.jcim,,p
GPT-Neo-2.7B,,2700000000.0,
LSTM(large)+Sememe+cell,,48000000.0,
Llama 4 Scout,,109000000000.0,
OmniParser: Icon Description Model,,,
Amber,,6700000000.0,
Midjourney V7,,,
Magistral Small 1.1,,24000000000.0,
Phi-4-Multimodal,,5600000000.0,
CarrotAI (CarrotAI大模型),,7000000000.0,
Microsoft MAI-1,,500000000000.0,
GLM-4-32B-0414,,32000000000.0,
GLM-4V-9B,,9000000000.0,
Gemini 2.5 Pro,,,
Unified-IO (XL),,2925000000.0,
LMRec,,210000000.0,
gpt-image-1,,,
SantaCoder,,1100000000.0,
Pythia-1b,,1000000000.0,
DTN (Domain Transfer Network),,,
Qwen3-235B-A22B,,235000000000.0,
gpt-sw3-40b,,40000000000.0,
MetNet,,225000000.0,
AWD-FWM (WT2),,37000000.0,
GPT-Neo-2.7B (finetuned on PTB),,2700000000.0,
Baigong (百工),,,
DNN EM segmentation,,218896.0,
DeepSeek-VL-7B,,7000000000.0,
Playground v3,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),,33000000.0,
PepNet,,,
T-Lite,,7000000000.0,
Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-Terminal Coding Sequences,acssynbio,,p
EfficientNetV2-XL,,208000000.0,
Talent Search and Recommendation Systems,,,
Mamba-24M (SC09),,23400000.0,
CMS-RCNN,,138000000.0,
SeedEdit,,,
Skywork-13B,,13000000000.0,
MMAPLE,,,
DeepSpeech2 (English),,38000000.0,
Wu Dao - Wen Hui,,11300000000.0,
ProTeM,,,
AWD-LSTM + Phrase Induction + finetuning (PTB),,24000000.0,
RNA-FM,,,
mathstral,,,
AWD-LSTM + DeFINE,,20000000.0,
OPT-125M (finetuned on PTB),,125000000.0,
Deep rectifier networks,,,
"GPT-2 (117M, SLW 110K)",,117000000.0,
LearnLM-Tutor,,,
AutoGLM Rumination,chinese-ai-firm-launches-model-claims-to-outperform-deepseek,, 2025-04-01
Stable Diffusion 1.1,,,
POKE´LLMON,,,
Hierarchical Scene Labeling (Stanford Background),,51609600.0,
vScreenML 2.0,,,
PaliGemma 2 3B Mix 448,,2920000000.0,
EXAONE Deep 32B,,32000000000.0,
Hybrid H3-2.7B,,2700000000.0,
Hopfield Networks (2020),,,
FragLlama,,779000000.0,
CNN Committee (NIST),,128420.0,
DeepSeek Coder 33B,,33000000000.0,
Vector Space Model,,255000.0,
EI-REHN-1200D (PTB),,25000000.0,
ESM1v,,650000000.0,
GRUs,,,
Differentiable neural computer,,,
Selective Search,,,
VRMBG 2.0,,,
Baize (白泽),,,
Aya,,13000000000.0,
xTrimoPGLM -100B,,100000000000.0,
phi-3-medium 14B,,14000000000.0,
XVERSE-MoE-A4.2B,,4200000000.0,
GPT3-2.7B (FlashAttention-2),,2700000000.0,
xGen-MM (BLIP-3),,4000000000.0,
AlphaZero,,,
Qwen2.5-Coder (32B),,32500000000.0,
DreamerV3,,200000000.0,
ReLU (LFW),Image 1: Icon for dl.acm.org](https://dl.acm.org/favicon.ico,,.
ISR network,,,
Sonar Reasoning Pro,,,
Uni-Mol Molecular Model,,,
Transformer - LibriVox + Decoding/Rescoring,,296000000.0,
Mogrifier RLSTM (PTB),,24000000.0,
Brain2Qwerty,,400000000.0,
Gemma 2 9B,,9000000000.0,
Digest: Cyber AI Analyst,,,
Marco-o1,,7000000000.0,
MetaMath 70B,,70000000000.0,
Claude 1.3,,,
ABAB,,,
ESM2-3B," Just a moment...

URL Source: https://www.science.org/doi/abs/10.1126/science.ade2574",3000000000.0,.
ReALM,,3000000000.0,
Sandwich Transformer,,209000000.0,
PeptideBERT,,,
Wan 2.2 14B T2V,,14000000000.0,
DBRX,,132000000000.0,
ESMFlow,,,
AWD-LSTM-DOC (fin) (37M),,37000000.0,
Llama-3.1-Minitron-4B,,4000000000.0,
Contriever,,110000000.0,
Gen-4 Turbo,,,
S + I-Attention (3),,,
Alpaca,,7000000000.0,
Mixture of linear models,,384000.0,
Gemini-Exp-1114,,,
DeepNash,[Image 1: Icon for www.science.org](https://www.science.org/favicon.ico,,.science.org/stoken/author-tokens/ST-887/full
phi-3.5-Vision,,4200000000.0,
DistBelief Speech,,47185920.0,
Oryx 7B,,,
Densely Connected LSTM + Var. Dropout,,23000000.0,
SeedLLM,,7000000000.0,
PepINVENT,,,
"Cosmos-1.0-
Diffusion-14B Video2World",,14000000000.0,
Grok 4,,,
AWD-FWM (PTB),,24000000.0,
JetFire (GPT2-LARGE),,774000000.0,
MLDD3UTRmRRNAS,,44000000.0,
Segment Anything Model,,636000000.0,
PLLaMa,,13000000000.0,
Phoenix 1.0 Ultra,,,
RWKV-4 World (7B),,7393000000.0,
Libratus, HTTPSConnectionPool,, Read timed out
phi-3-mini 3.8B,,3800000000.0,
BLOOM-7.1B,,7070000000.0,
SANA 1.6B,,1648000000.0,
Mid-level Features,,,
Mistral Large,,,
Hybrid CNN/SVM Object Categorizer,,3590057.0,
BiosimDock,,,
Mengzi-Lite,,,
Llama 3-TAIDE-LX-8B-Chat-Alpha1,,8000000000.0,
Stable Diffusion 1.2,,,
ResNet-200,,,
OtterHD-8B,,8000000000.0,
EXAONE Deep 7.8B,,7800000000.0,
Stable Cascade,,5120000000.0,
Mengzi-Fin-7B,,7000000000.0,
Mengzi-Fin-13B, HTTPSConnectionPool,13000000000.0," HTTPSConnectionPool(host='hub.baai.ac.cn', port=443): Read timed out"
Qwen2-72B,,72710000000.0,
Granite 20B,,20000000000.0,
PaLM-2 Unicorn,,,
Fish-Speech 1.4,,,
CoPRA,,,
PocketFlow,,,
LongVU,,7000000000.0,
SNM-skip,,62000000000.0,
CHIEF,,,
Q-learning,,,
MLP as Bayesian Approximator,,,
SYNTERACT,,420000000.0,
LSTM-Char-Large,,19000000.0,
Kokoro v0.19,,82000000.0,
Poolside Malibu,,,
ERNIE-4.5-0.3B,,360000000.0,
RNN + char2-MS-vec,,158000000.0,
Phi-1,,1300000000.0,
Movie Gen Video,,30000000000.0,
Fugatto 1,,2500000000.0,
MADLAD-400 10B,,10700000000.0,
ProteinMPNN, https://www.science.org/doi/full/10.1126/science.add2187,,.
Seq2Seq LSTM,,1920000000.0,
LFM 40B,,40300000000.0,
Step-1.5V,,100000000000.0,
Jurassic-1-Jumbo,,178000000000.0,
Nemotron-4 15B,,15000000000.0,
Hermes 2 Theta Llama-3 70B,,70000000000.0,
NPD,,313856.0,
T2V-Turbo-v2,,,
text-embedding-3-small,,,
MathGPT,,,
ProtBERT-BFD,,420000000.0,
EXAONE 3.5 7.8B,,7800000000.0,
TRIMELMext (247M),,247000000.00000003,
NASv3 (CIFAR-10),,37400000.0,
Fraternal dropout + AWD-LSTM 3-layer (PTB),,24000000.0,
SpecAugment,,,
QVQ,,72000000000.0,
WD+LR+M,,,
BADGER,,2900000.0,
LTE speaker verification system,,2061.0,
DEQ-Transformer (Post-LN) + Jacobian Regularisation,,98000000.0,
Vicuna-13B-v1.5,,13000000000.0,
Cosine Genie,,,
GAWWN,,,
Typhoon 2.1 Gemma 4B,,4000000000.0,
Seed Prover,,,
Ministral 3B,,3000000000.0,
DeepSeek-R1-Lite-Preview,,,
Tranception,,700000000.0,
rStar-Math (Qwen2-Math-7B base),,7000000000.0,
OpenChat-13b,,13000000000.0,
BindDM,,,
P-LLama3,,8000000000.0,
PNASNet-5,,86100000.0,
rStar-Math (Qwen2.5-Math-7B base),,7000000000.0,
Qwen2.5-7B,,7610000000.0,
Claude 3 Sonnet,,,
StyleTTS 2,,,
AlphaFold 2,,93000000.0,
Command R,,35000000000.0,
ProteinChat,,14000000000.0,
BERT-Large-CAS (WT103),,340000000.0,
PIXART-α,,600000000.0,
ProBERTa,,44000000.0,
Yi-1.5-34B,,34000000000.0,
Show-1,,,
Magic LLM (魔法大模型),,7000000000.0,
ADM,,559000000.0,
Doubao-lite,,,
Agile Soccer Robot,,,
ALIGN,,820000000.0,
Neural cache model (size=2000),,,
DreamLLM,,7000000000.0,
Gemini Robotics-ER,,,
LLaVA,,13000000000.0,
LongT5,,3000000000.0,
Index-1.9B,,1900000000.0,
DCN+,,,
Stable Diffusion 3,,8000000000.0,
Qwen2.5-VL-7B ,,7000000000.0,
OLMo-7B,,7000000000.0,
CogVideoX,,5000000000.0,
Multi-Token Prediction 7B,,6700000000.0,
Qiyuan 3.0,Sci-tech Innovation Powers High-quality Development,,3000
MetaLM,,,
GPT-Neo-2.7B (finetuned),,2700000000.0,
Emu2,,37000000000.0,
STEPS,,,
Youtube recommendation model,,,
Hide and Seek,,1600000.0,
HJRSS,,16000000.0,
CALM,,,
RiboCode,,,
Binarized Neural Network (MNIST),,37000000.0,
Gemma 3n,,7850000000.0,
DDGemb,,,
Chemistry42,acs.jcim,,pubs.acs.org/doi/full/10.1021/acs.jcim.2c01191
SPN (CelebA HQ),,50000000.0,
GeoSeqBuilder,,,
DLRM-12T,,12000000000000.0,
MPT-7B,,7000000000.0,
PaLM-2 Gecko,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
